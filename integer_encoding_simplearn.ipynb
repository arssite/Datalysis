{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGxNVPYYwlgx152HPSLzS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arssite/Datalysis/blob/main/integer_encoding_simplearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recurrent Neural Network (RNN)**\n",
        "RNN is a type of neural network that is designed to recognize patterns in sequences of data. RNNs are able to do this by feeding back the output of a previous layer into the input of the next layer. This allows the network to learn long-term dependencies in the data.\n",
        "\n",
        "RNNs are typically used for tasks such as:\n",
        "\n",
        "Natural language processing: RNNs can be used to understand the meaning of sentences and to generate text.\n",
        "Speech recognition: RNNs can be used to recognize spoken words.\n",
        "Machine translation: RNNs can be used to translate text from one language to another.\n",
        "Time series forecasting: RNNs can be used to predict future values of a time series.\n",
        "RNNs are a powerful tool for learning from sequential data. However, they can also be difficult to train. One of the challenges with RNNs is that they can suffer from the vanishing gradient problem. This problem occurs when the gradients of the error function become very small, which can make it difficult for the network to learn.\n",
        "\n",
        "There are a number of different types of RNNs, including:\n",
        "\n",
        "Simple Recurrent Neural Networks (SRNs): SRNs are the simplest type of RNNs. They have a single hidden layer that is connected to the input and output layers.\n",
        "Long Short-Term Memory (LSTM) networks: LSTMs are a type of RNN that is designed to address the vanishing gradient problem. LSTMs have a special type of memory cell that can store information over long periods of time.\n",
        "Gated Recurrent Unit (GRU) networks: GRUs are a type of RNN that is similar to LSTMs. GRUs have a simpler architecture than LSTMs, but they can still perform well on a variety of tasks.\n",
        "RNNs are a powerful tool for learning from sequential data. They have been used to achieve state-of-the-art results on a wide range of tasks."
      ],
      "metadata": {
        "id": "FVcBhCfHqKTC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0oDOSwchep4E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "docs = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
        "    \"Python is a high-level programming language.\",\n",
        "    \"Stack Overflow is a question and answer website for professional and enthusiast programmers.\",\n",
        "    \"Artificial intelligence is reshaping industries across the globe.\",\n",
        "    \"The universe is vast and full of mysteries.\",\n",
        "    \"Climate change is a pressing issue that requires global cooperation.\",\n",
        "    \"The sun rises in the east and sets in the west.\",\n",
        "    \"Music has the power to evoke strong emotions.\",\n",
        "    \"Education is the key to unlocking opportunities.\",\n",
        "    \"Health is wealth.\",\n",
        "    \"The journey of a thousand miles begins with a single step.\",\n",
        "    \"Reading is to the mind what exercise is to the body.\",\n",
        "    \"Life is like a box of chocolates; you never know what you're gonna get.\",\n",
        "    \"Laughter is the best medicine.\",\n",
        "    \"The only way to do great work is to love what you do.\",\n",
        "    \"Success is not final, failure is not fatal: It is the courage to continue that counts.\",\n",
        "    \"Believe you can and you're halfway there.\",\n",
        "    \"Yesterday is history, tomorrow is a mystery, but today is a gift. That is why it is called the present.\",\n",
        "    \"Happiness is not something ready made. It comes from your own actions.\",\n",
        "    \"Be yourself; everyone else is already taken.\",\n",
        "    \"In three words I can sum up everything I've learned about life: it goes on.\",\n",
        "    \"To be yourself in a world that is constantly trying to make you something else is the greatest accomplishment.\",\n",
        "    \"Life is either a daring adventure or nothing at all.\",\n",
        "    \"Success is not the key to happiness. Happiness is the key to success. If you love what you are doing, you will be successful.\",\n",
        "    \"Don't cry because it's over, smile because it happened.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "token=Tokenizer(oov_token='<nothing')"
      ],
      "metadata": {
        "id": "0rtCyaNzvJkZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Tokenizer class in Keras is a text tokenization utility. It allows you to vectorize a text corpus into a list of integers, where each integer represents a token (word or character). This is a common preprocessing step in natural language processing (NLP) tasks, especially when working with neural networks."
      ],
      "metadata": {
        "id": "Dqrudr6bvyVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Sample text corpus\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
        "    \"Python is a high-level programming language.\"\n",
        "]\n",
        "\n",
        "# Create a tokenizer object\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Print the word index mapping\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Word index:\", word_index)\n",
        "\n",
        "# Print the sequences\n",
        "print(\"Sequences:\", sequences)\n",
        "\n",
        "______________________________________\n",
        "Word index: {'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8, 'lorem': 9, 'ipsum': 10, 'dolor': 11, 'sit': 12, 'amet': 13, 'consectetur': 14, 'adipiscing': 15, 'elit': 16, 'python': 17, 'is': 18, 'a': 19, 'high': 20, 'level': 21, 'programming': 22, 'language': 23}\n",
        "Sequences: [[1, 2, 3, 4, 5, 6, 1, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16], [17, 18, 19, 20, 21, 22, 23]]'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "dL8VWow7vQEQ",
        "outputId": "f06c47d2-85ee-49c8-9750-94b9f06e2bc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom keras.preprocessing.text import Tokenizer\\n\\n# Sample text corpus\\ntexts = [\\n    \"The quick brown fox jumps over the lazy dog.\",\\n    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\\n    \"Python is a high-level programming language.\"\\n]\\n\\n# Create a tokenizer object\\ntokenizer = Tokenizer()\\n\\n# Fit the tokenizer on the texts\\ntokenizer.fit_on_texts(texts)\\n\\n# Convert text to sequences of integers\\nsequences = tokenizer.texts_to_sequences(texts)\\n\\n# Print the word index mapping\\nword_index = tokenizer.word_index\\nprint(\"Word index:\", word_index)\\n\\n# Print the sequences\\nprint(\"Sequences:\", sequences)\\n\\n______________________________________\\nWord index: {\\'the\\': 1, \\'quick\\': 2, \\'brown\\': 3, \\'fox\\': 4, \\'jumps\\': 5, \\'over\\': 6, \\'lazy\\': 7, \\'dog\\': 8, \\'lorem\\': 9, \\'ipsum\\': 10, \\'dolor\\': 11, \\'sit\\': 12, \\'amet\\': 13, \\'consectetur\\': 14, \\'adipiscing\\': 15, \\'elit\\': 16, \\'python\\': 17, \\'is\\': 18, \\'a\\': 19, \\'high\\': 20, \\'level\\': 21, \\'programming\\': 22, \\'language\\': 23}\\nSequences: [[1, 2, 3, 4, 5, 6, 1, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16], [17, 18, 19, 20, 21, 22, 23]]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "aarDhK3Cv4zR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer.word_index attribute provides a dictionary mapping of words to their respective integer indices. This mapping is built when you call the fit_on_texts() method of the Tokenizer class. This dictionary is useful because it allows you to convert words to their corresponding integer indices and vice versa."
      ],
      "metadata": {
        "id": "GWBZkDiVwvTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgkKeC2vwG1s",
        "outputId": "11176405-12f3-48d6-de71-d92d6e0dacf9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<nothing': 1,\n",
              " 'is': 2,\n",
              " 'the': 3,\n",
              " 'to': 4,\n",
              " 'a': 5,\n",
              " 'you': 6,\n",
              " 'and': 7,\n",
              " 'it': 8,\n",
              " 'that': 9,\n",
              " 'in': 10,\n",
              " 'what': 11,\n",
              " 'not': 12,\n",
              " 'of': 13,\n",
              " 'key': 14,\n",
              " 'life': 15,\n",
              " 'success': 16,\n",
              " 'happiness': 17,\n",
              " 'be': 18,\n",
              " 'over': 19,\n",
              " \"you're\": 20,\n",
              " 'do': 21,\n",
              " 'love': 22,\n",
              " 'can': 23,\n",
              " 'something': 24,\n",
              " 'yourself': 25,\n",
              " 'else': 26,\n",
              " 'because': 27,\n",
              " 'quick': 28,\n",
              " 'brown': 29,\n",
              " 'fox': 30,\n",
              " 'jumps': 31,\n",
              " 'lazy': 32,\n",
              " 'dog': 33,\n",
              " 'lorem': 34,\n",
              " 'ipsum': 35,\n",
              " 'dolor': 36,\n",
              " 'sit': 37,\n",
              " 'amet': 38,\n",
              " 'consectetur': 39,\n",
              " 'adipiscing': 40,\n",
              " 'elit': 41,\n",
              " 'python': 42,\n",
              " 'high': 43,\n",
              " 'level': 44,\n",
              " 'programming': 45,\n",
              " 'language': 46,\n",
              " 'stack': 47,\n",
              " 'overflow': 48,\n",
              " 'question': 49,\n",
              " 'answer': 50,\n",
              " 'website': 51,\n",
              " 'for': 52,\n",
              " 'professional': 53,\n",
              " 'enthusiast': 54,\n",
              " 'programmers': 55,\n",
              " 'artificial': 56,\n",
              " 'intelligence': 57,\n",
              " 'reshaping': 58,\n",
              " 'industries': 59,\n",
              " 'across': 60,\n",
              " 'globe': 61,\n",
              " 'universe': 62,\n",
              " 'vast': 63,\n",
              " 'full': 64,\n",
              " 'mysteries': 65,\n",
              " 'climate': 66,\n",
              " 'change': 67,\n",
              " 'pressing': 68,\n",
              " 'issue': 69,\n",
              " 'requires': 70,\n",
              " 'global': 71,\n",
              " 'cooperation': 72,\n",
              " 'sun': 73,\n",
              " 'rises': 74,\n",
              " 'east': 75,\n",
              " 'sets': 76,\n",
              " 'west': 77,\n",
              " 'music': 78,\n",
              " 'has': 79,\n",
              " 'power': 80,\n",
              " 'evoke': 81,\n",
              " 'strong': 82,\n",
              " 'emotions': 83,\n",
              " 'education': 84,\n",
              " 'unlocking': 85,\n",
              " 'opportunities': 86,\n",
              " 'health': 87,\n",
              " 'wealth': 88,\n",
              " 'journey': 89,\n",
              " 'thousand': 90,\n",
              " 'miles': 91,\n",
              " 'begins': 92,\n",
              " 'with': 93,\n",
              " 'single': 94,\n",
              " 'step': 95,\n",
              " 'reading': 96,\n",
              " 'mind': 97,\n",
              " 'exercise': 98,\n",
              " 'body': 99,\n",
              " 'like': 100,\n",
              " 'box': 101,\n",
              " 'chocolates': 102,\n",
              " 'never': 103,\n",
              " 'know': 104,\n",
              " 'gonna': 105,\n",
              " 'get': 106,\n",
              " 'laughter': 107,\n",
              " 'best': 108,\n",
              " 'medicine': 109,\n",
              " 'only': 110,\n",
              " 'way': 111,\n",
              " 'great': 112,\n",
              " 'work': 113,\n",
              " 'final': 114,\n",
              " 'failure': 115,\n",
              " 'fatal': 116,\n",
              " 'courage': 117,\n",
              " 'continue': 118,\n",
              " 'counts': 119,\n",
              " 'believe': 120,\n",
              " 'halfway': 121,\n",
              " 'there': 122,\n",
              " 'yesterday': 123,\n",
              " 'history': 124,\n",
              " 'tomorrow': 125,\n",
              " 'mystery': 126,\n",
              " 'but': 127,\n",
              " 'today': 128,\n",
              " 'gift': 129,\n",
              " 'why': 130,\n",
              " 'called': 131,\n",
              " 'present': 132,\n",
              " 'ready': 133,\n",
              " 'made': 134,\n",
              " 'comes': 135,\n",
              " 'from': 136,\n",
              " 'your': 137,\n",
              " 'own': 138,\n",
              " 'actions': 139,\n",
              " 'everyone': 140,\n",
              " 'already': 141,\n",
              " 'taken': 142,\n",
              " 'three': 143,\n",
              " 'words': 144,\n",
              " 'i': 145,\n",
              " 'sum': 146,\n",
              " 'up': 147,\n",
              " 'everything': 148,\n",
              " \"i've\": 149,\n",
              " 'learned': 150,\n",
              " 'about': 151,\n",
              " 'goes': 152,\n",
              " 'on': 153,\n",
              " 'world': 154,\n",
              " 'constantly': 155,\n",
              " 'trying': 156,\n",
              " 'make': 157,\n",
              " 'greatest': 158,\n",
              " 'accomplishment': 159,\n",
              " 'either': 160,\n",
              " 'daring': 161,\n",
              " 'adventure': 162,\n",
              " 'or': 163,\n",
              " 'nothing': 164,\n",
              " 'at': 165,\n",
              " 'all': 166,\n",
              " 'if': 167,\n",
              " 'are': 168,\n",
              " 'doing': 169,\n",
              " 'will': 170,\n",
              " 'successful': 171,\n",
              " \"don't\": 172,\n",
              " 'cry': 173,\n",
              " \"it's\": 174,\n",
              " 'smile': 175,\n",
              " 'happened': 176}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token.word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXjbOwSdwLSg",
        "outputId": "775df047-a268-4b68-c892-3e5b132b5b8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('the', 19),\n",
              "             ('quick', 1),\n",
              "             ('brown', 1),\n",
              "             ('fox', 1),\n",
              "             ('jumps', 1),\n",
              "             ('over', 2),\n",
              "             ('lazy', 1),\n",
              "             ('dog', 1),\n",
              "             ('lorem', 1),\n",
              "             ('ipsum', 1),\n",
              "             ('dolor', 1),\n",
              "             ('sit', 1),\n",
              "             ('amet', 1),\n",
              "             ('consectetur', 1),\n",
              "             ('adipiscing', 1),\n",
              "             ('elit', 1),\n",
              "             ('python', 1),\n",
              "             ('is', 27),\n",
              "             ('a', 10),\n",
              "             ('high', 1),\n",
              "             ('level', 1),\n",
              "             ('programming', 1),\n",
              "             ('language', 1),\n",
              "             ('stack', 1),\n",
              "             ('overflow', 1),\n",
              "             ('question', 1),\n",
              "             ('and', 5),\n",
              "             ('answer', 1),\n",
              "             ('website', 1),\n",
              "             ('for', 1),\n",
              "             ('professional', 1),\n",
              "             ('enthusiast', 1),\n",
              "             ('programmers', 1),\n",
              "             ('artificial', 1),\n",
              "             ('intelligence', 1),\n",
              "             ('reshaping', 1),\n",
              "             ('industries', 1),\n",
              "             ('across', 1),\n",
              "             ('globe', 1),\n",
              "             ('universe', 1),\n",
              "             ('vast', 1),\n",
              "             ('full', 1),\n",
              "             ('of', 3),\n",
              "             ('mysteries', 1),\n",
              "             ('climate', 1),\n",
              "             ('change', 1),\n",
              "             ('pressing', 1),\n",
              "             ('issue', 1),\n",
              "             ('that', 4),\n",
              "             ('requires', 1),\n",
              "             ('global', 1),\n",
              "             ('cooperation', 1),\n",
              "             ('sun', 1),\n",
              "             ('rises', 1),\n",
              "             ('in', 4),\n",
              "             ('east', 1),\n",
              "             ('sets', 1),\n",
              "             ('west', 1),\n",
              "             ('music', 1),\n",
              "             ('has', 1),\n",
              "             ('power', 1),\n",
              "             ('to', 11),\n",
              "             ('evoke', 1),\n",
              "             ('strong', 1),\n",
              "             ('emotions', 1),\n",
              "             ('education', 1),\n",
              "             ('key', 3),\n",
              "             ('unlocking', 1),\n",
              "             ('opportunities', 1),\n",
              "             ('health', 1),\n",
              "             ('wealth', 1),\n",
              "             ('journey', 1),\n",
              "             ('thousand', 1),\n",
              "             ('miles', 1),\n",
              "             ('begins', 1),\n",
              "             ('with', 1),\n",
              "             ('single', 1),\n",
              "             ('step', 1),\n",
              "             ('reading', 1),\n",
              "             ('mind', 1),\n",
              "             ('what', 4),\n",
              "             ('exercise', 1),\n",
              "             ('body', 1),\n",
              "             ('life', 3),\n",
              "             ('like', 1),\n",
              "             ('box', 1),\n",
              "             ('chocolates', 1),\n",
              "             ('you', 7),\n",
              "             ('never', 1),\n",
              "             ('know', 1),\n",
              "             (\"you're\", 2),\n",
              "             ('gonna', 1),\n",
              "             ('get', 1),\n",
              "             ('laughter', 1),\n",
              "             ('best', 1),\n",
              "             ('medicine', 1),\n",
              "             ('only', 1),\n",
              "             ('way', 1),\n",
              "             ('do', 2),\n",
              "             ('great', 1),\n",
              "             ('work', 1),\n",
              "             ('love', 2),\n",
              "             ('success', 3),\n",
              "             ('not', 4),\n",
              "             ('final', 1),\n",
              "             ('failure', 1),\n",
              "             ('fatal', 1),\n",
              "             ('it', 5),\n",
              "             ('courage', 1),\n",
              "             ('continue', 1),\n",
              "             ('counts', 1),\n",
              "             ('believe', 1),\n",
              "             ('can', 2),\n",
              "             ('halfway', 1),\n",
              "             ('there', 1),\n",
              "             ('yesterday', 1),\n",
              "             ('history', 1),\n",
              "             ('tomorrow', 1),\n",
              "             ('mystery', 1),\n",
              "             ('but', 1),\n",
              "             ('today', 1),\n",
              "             ('gift', 1),\n",
              "             ('why', 1),\n",
              "             ('called', 1),\n",
              "             ('present', 1),\n",
              "             ('happiness', 3),\n",
              "             ('something', 2),\n",
              "             ('ready', 1),\n",
              "             ('made', 1),\n",
              "             ('comes', 1),\n",
              "             ('from', 1),\n",
              "             ('your', 1),\n",
              "             ('own', 1),\n",
              "             ('actions', 1),\n",
              "             ('be', 3),\n",
              "             ('yourself', 2),\n",
              "             ('everyone', 1),\n",
              "             ('else', 2),\n",
              "             ('already', 1),\n",
              "             ('taken', 1),\n",
              "             ('three', 1),\n",
              "             ('words', 1),\n",
              "             ('i', 1),\n",
              "             ('sum', 1),\n",
              "             ('up', 1),\n",
              "             ('everything', 1),\n",
              "             (\"i've\", 1),\n",
              "             ('learned', 1),\n",
              "             ('about', 1),\n",
              "             ('goes', 1),\n",
              "             ('on', 1),\n",
              "             ('world', 1),\n",
              "             ('constantly', 1),\n",
              "             ('trying', 1),\n",
              "             ('make', 1),\n",
              "             ('greatest', 1),\n",
              "             ('accomplishment', 1),\n",
              "             ('either', 1),\n",
              "             ('daring', 1),\n",
              "             ('adventure', 1),\n",
              "             ('or', 1),\n",
              "             ('nothing', 1),\n",
              "             ('at', 1),\n",
              "             ('all', 1),\n",
              "             ('if', 1),\n",
              "             ('are', 1),\n",
              "             ('doing', 1),\n",
              "             ('will', 1),\n",
              "             ('successful', 1),\n",
              "             (\"don't\", 1),\n",
              "             ('cry', 1),\n",
              "             ('because', 2),\n",
              "             (\"it's\", 1),\n",
              "             ('smile', 1),\n",
              "             ('happened', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token.document_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BohQYfdw-pI",
        "outputId": "5b8de21c-ebe4-484b-b404-d31bec3932d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq=token.texts_to_sequences(docs)\n",
        "seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlv59gLDxc85",
        "outputId": "8216f850-1aa1-47b8-e307-8f0f1cab7974"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 28, 29, 30, 31, 19, 3, 32, 33],\n",
              " [34, 35, 36, 37, 38, 39, 40, 41],\n",
              " [42, 2, 5, 43, 44, 45, 46],\n",
              " [47, 48, 2, 5, 49, 7, 50, 51, 52, 53, 7, 54, 55],\n",
              " [56, 57, 2, 58, 59, 60, 3, 61],\n",
              " [3, 62, 2, 63, 7, 64, 13, 65],\n",
              " [66, 67, 2, 5, 68, 69, 9, 70, 71, 72],\n",
              " [3, 73, 74, 10, 3, 75, 7, 76, 10, 3, 77],\n",
              " [78, 79, 3, 80, 4, 81, 82, 83],\n",
              " [84, 2, 3, 14, 4, 85, 86],\n",
              " [87, 2, 88],\n",
              " [3, 89, 13, 5, 90, 91, 92, 93, 5, 94, 95],\n",
              " [96, 2, 4, 3, 97, 11, 98, 2, 4, 3, 99],\n",
              " [15, 2, 100, 5, 101, 13, 102, 6, 103, 104, 11, 20, 105, 106],\n",
              " [107, 2, 3, 108, 109],\n",
              " [3, 110, 111, 4, 21, 112, 113, 2, 4, 22, 11, 6, 21],\n",
              " [16, 2, 12, 114, 115, 2, 12, 116, 8, 2, 3, 117, 4, 118, 9, 119],\n",
              " [120, 6, 23, 7, 20, 121, 122],\n",
              " [123,\n",
              "  2,\n",
              "  124,\n",
              "  125,\n",
              "  2,\n",
              "  5,\n",
              "  126,\n",
              "  127,\n",
              "  128,\n",
              "  2,\n",
              "  5,\n",
              "  129,\n",
              "  9,\n",
              "  2,\n",
              "  130,\n",
              "  8,\n",
              "  2,\n",
              "  131,\n",
              "  3,\n",
              "  132],\n",
              " [17, 2, 12, 24, 133, 134, 8, 135, 136, 137, 138, 139],\n",
              " [18, 25, 140, 26, 2, 141, 142],\n",
              " [10, 143, 144, 145, 23, 146, 147, 148, 149, 150, 151, 15, 8, 152, 153],\n",
              " [4, 18, 25, 10, 5, 154, 9, 2, 155, 156, 4, 157, 6, 24, 26, 2, 3, 158, 159],\n",
              " [15, 2, 160, 5, 161, 162, 163, 164, 165, 166],\n",
              " [16,\n",
              "  2,\n",
              "  12,\n",
              "  3,\n",
              "  14,\n",
              "  4,\n",
              "  17,\n",
              "  17,\n",
              "  2,\n",
              "  3,\n",
              "  14,\n",
              "  4,\n",
              "  16,\n",
              "  167,\n",
              "  6,\n",
              "  22,\n",
              "  11,\n",
              "  6,\n",
              "  168,\n",
              "  169,\n",
              "  6,\n",
              "  170,\n",
              "  18,\n",
              "  171],\n",
              " [172, 173, 27, 174, 19, 175, 27, 8, 176]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXlJ25YlxjII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}