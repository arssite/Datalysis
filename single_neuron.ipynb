{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjMfxxbCzFZIN57LFJfo9l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arssite/Datalysis/blob/main/single_neuron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single neuron is a fundamental unit of a neural network. It takes multiple inputs, multiplies them by corresponding weights, sums them up, adds a bias, and applies an activation function to produce an output.\n",
        "\n",
        "Steps involved:\n",
        "\n",
        "Input: The neuron receives multiple inputs (x1, x2, ... xn).\n",
        "Weights: Each input is associated with a weight (w1, w2, ... wn) representing its importance.\n",
        "Summation: The neuron calculates the weighted sum of inputs: (x1 * w1) + (x2 * w2) + ... + (xn * wn).\n",
        "Bias: A bias term (b) is added to the weighted sum to shift the activation function.\n",
        "Activation Function: A non-linear function (e.g., sigmoid, ReLU) is applied to the sum and bias to introduce non-linearity and determine the neuron's output.\n",
        "In real neural networks, multiple interconnected neurons form layers to process complex data."
      ],
      "metadata": {
        "id": "WQ_zdz0UYg-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y10e3ysb-JTh"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x is weightedsum here\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "64nNlLNPYV0w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    return sigmoid(total)"
      ],
      "metadata": {
        "id": "09gEcoLTYvWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternate without class"
      ],
      "metadata": {
        "id": "iOEBY0DOaVBQ"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "def neuron(inputs, weights, bias):\n",
        "  weighted_sum = np.dot(inputs, weights) + bias\n",
        "  output = 1 / (1 + np.exp(-weighted_sum))\n",
        "  return output"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HhMftOcoaP0X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g01JHpqdbzB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}