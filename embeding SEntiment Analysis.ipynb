{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arssite/Datalysis/blob/main/embeding%20SEntiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "It is a technique in deep learning used to represent categorical variables as vectors of real numbers. This allows the model to learn relationships between different categories and use them for prediction or classification tasks.\n",
        "\n",
        "Benefits of using embedding:\n",
        "\n",
        "Reduced dimensionality: Embeddings are typically lower-dimensional than the original categorical variables, which can improve computational efficiency and reduce overfitting.\n",
        "Improved interpretability: Embeddings can be visualized to understand the relationships between different categories.\n",
        "Increased flexibility: Embeddings can be used with a variety of deep learning models, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
        "How embedding works:\n",
        "\n",
        "One-hot encoding: The first step is to convert the categorical variables into one-hot encoded vectors. This means that each category is represented by a vector of zeros, except for the position corresponding to the category, which is set to 1.\n",
        "Embedding layer: The one-hot encoded vectors are then passed through an embedding layer. This layer is a neural network that learns a mapping from the one-hot encoded vectors to a lower-dimensional space.\n",
        "Output: The output of the embedding layer is a vector of real numbers for each category. These vectors can then be used as input to other deep learning models.\n",
        "Applications of embedding:\n",
        "\n",
        "Natural language processing: Embeddings are commonly used in natural language processing tasks such as sentiment analysis, text classification, and machine translation.\n",
        "Computer vision: Embeddings can also be used in computer vision tasks such as image classification and object detection.\n",
        "Recommendation systems: Embeddings can be used to learn user preferences and recommend items that the user is likely to be interested in."
      ],
      "metadata": {
        "id": "bouSs0h_vkG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yRHqshyNq4Zy"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = ['go india',\n",
        "'india india',\n",
        "'hip hip hurray',\n",
        "'jeetega bhai jeetega india jeetega',\n",
        " 'dhoni dhoni',\n",
        "  'modi ji ki jai']"
      ],
      "metadata": {
        "id": "Bhe9TTE2rMqq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<nothing>')"
      ],
      "metadata": {
        "id": "y5Gop-hQrMup"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "j2q24U0PrMyB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZsQis5IrM1B",
        "outputId": "044f2b52-5674-48fe-e536-e91eecfd0c40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(docs)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJwbfgjtrM35",
        "outputId": "bb914ef0-2173-4f9e-fecb-72d48fc710a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[6, 2], [2, 2], [4, 4, 7], [3, 8, 3, 2, 3], [5, 5], [9, 10, 11, 12]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "sequences = pad_sequences(sequences,padding='post')\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udMrUv1WrM7B",
        "outputId": "b7a9e718-03bb-464c-c050-876779598909"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  2,  0,  0,  0],\n",
              "       [ 2,  2,  0,  0,  0],\n",
              "       [ 4,  4,  7,  0,  0],\n",
              "       [ 3,  8,  3,  2,  3],\n",
              "       [ 5,  5,  0,  0,  0],\n",
              "       [ 9, 10, 11, 12,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
      ],
      "metadata": {
        "id": "lUuN61YZrM95"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(17,output_dim=2,input_length=5))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R09SoXbWrNBB",
        "outputId": "a0e4d4e0-623b-4db5-f902-5756c0783ba0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5, 2)              34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34 (136.00 Byte)\n",
            "Trainable params: 34 (136.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile('adam','accuracy')"
      ],
      "metadata": {
        "id": "VPT-kU6zrNEC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(sequences)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgPmTe8jrNHL",
        "outputId": "3df934a3-59d3-427a-efd2-4fe61eb9c4c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 100ms/step\n",
            "[[[-0.02002052 -0.01683366]\n",
            "  [ 0.01163037  0.00929499]\n",
            "  [ 0.04131334  0.04343636]\n",
            "  [ 0.04131334  0.04343636]\n",
            "  [ 0.04131334  0.04343636]]\n",
            "\n",
            " [[ 0.01163037  0.00929499]\n",
            "  [ 0.01163037  0.00929499]\n",
            "  [ 0.04131334  0.04343636]\n",
            "  [ 0.04131334  0.04343636]\n",
            "  [ 0.04131334  0.04343636]]\n",
            "\n",
            " [[-0.04376053 -0.01810068]\n",
            "  [-0.04376053 -0.01810068]\n",
            "  [-0.00493211 -0.03661735]\n",
            "  [ 0.04131334  0.04343636]\n",
            "  [ 0.04131334  0.04343636]]\n",
            "\n",
            " [[-0.01182166  0.00794808]\n",
            "  [ 0.02736005  0.01168492]\n",
            "  [-0.01182166  0.00794808]\n",
            "  [ 0.01163037  0.00929499]\n",
            "  [-0.01182166  0.00794808]]\n",
            "\n",
            " [[ 0.02485165 -0.01045794]\n",
            "  [ 0.02485165 -0.01045794]\n",
            "  [ 0.04131334  0.04343636]\n",
            "  [ 0.04131334  0.04343636]\n",
            "  [ 0.04131334  0.04343636]]\n",
            "\n",
            " [[ 0.01195934  0.0227996 ]\n",
            "  [ 0.00754249 -0.04551395]\n",
            "  [-0.03544598  0.00313037]\n",
            "  [ 0.04069949 -0.04232335]\n",
            "  [ 0.04131334  0.04343636]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
      ],
      "metadata": {
        "id": "_0WFt4ulrNKK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=imdb.load_data()"
      ],
      "metadata": {
        "id": "9ficuBpwrNNi",
        "outputId": "a3e2423d-0a94-4650-cb6a-e310158f3c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=pad_sequences(x_train,padding='post',maxlen=50)"
      ],
      "metadata": {
        "id": "G5OxehqWrNQ6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test=pad_sequences(x_test,padding='post',maxlen=50)"
      ],
      "metadata": {
        "id": "Oxw7ul9z9LCL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cb70lgoi9VVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}